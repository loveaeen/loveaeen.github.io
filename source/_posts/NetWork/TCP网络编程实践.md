---
title: TCP网络编程实践
date: 2022-04-01 15:32:33
categories: NetWork
tags: 
typora-root-url: ../../
---

本文不会涉及TCP的各个基础知识点，主要是总结一些TCP网络编程实践中可能碰到的一些问题，以及相应的经过实践验证的解决方案等。虽然本文档很多细节主要是针对于Linux系统，不过，大部分建议适合于所有系统。

## 1. 服务端监听设置SO_REUSEADDR选项

当我们重启服务端程序的时候可能会碰到 “address already in use” 这样的报错信息，即地址已被使用，导致程序无法快速成功重启。老的进程关闭退出了，为什么还会报地址已被使用呢？

我们先来理解如下两点：

- TCP连接主动关闭方存在持续2MSL的`TIME_WAIT`状态；
- TCP连接由是由四元组<本地地址，本地端口，远程地址，远程端口>来确定的。

我们先简单回顾一下TCP连接关闭过程中的`TIME_WAIT`状态，如下所示：

<img src="/image/TCP%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5/tcp_close.png" style="zoom:50%;" />

TIME_WAIT存在的意义主要有两点：

1. 维护连接状态，使TCP连接能够可靠地关闭。如果连接主动关闭端发送的最后一条ACK丢失，连接被动关闭端会重传FIN报文。因此，主动关闭方必须维持连接状态，以支持收到重传的FIN后再次发送ACK。如果没有`TIME_WAIT`，并且最后一个ACK丢失，那么此时被动关闭端还会处于`LAST_ACK`一段时间，并等待重传；如果此时主动关闭方又立即创建新TCP连接且恰好使用了相同的四元组，连接会创建失败，会被对端重置。
2. 等待网络中所有此连接老的重复的、走失的报文消亡，避免此类报文对新的相同四元组的TCP连接造成干扰，因为这些报文的序号可能恰好落在新连接的接收窗口内。

因为每个TCP报文最大存活时间为MSL，一个往返最大是2*MSL，所以`TIME_WAIT`需要等待2MSL。

当进程关闭时，进程会发起连接的主动关闭，连接最后会进入`TIME_WAIT`状态。当新进程bind监听端口时，就会报错，因为有对应本地端口的连接还处于`TIME_WAIT`状态。

实际上，只有当新的TCP连接和老的TCP连接四元组完全一致，且老的迷走的报文序号落在新连接的接收窗口内时，才会造成干扰。为了使用`TIME_WAIT`状态的端口，现在大部分系统的实现都做了相关改进与扩展：

- 新连接SYN告知的初始序列号，要求一定要比`TIME_WAIT`状态老连接的序列号大，可以一定程度保证不会与老连接的报文序列号重叠。
- 开启TCP [timestamps扩展选项](https://datatracker.ietf.org/doc/html/rfc6191)后，新连接的时间戳要求一定要比`TIME_WAIT`状态老连接的时间戳大，可以保证老连接的报文不会影响新连接。

因此，在开启了TCP timestamps扩展选项的情况下（`net.ipv4.tcp_timestamps = 1`），可以放心的设置`SO_REUSEADDR`选项，支持程序快速重启。

注意不要与`net.ipv4.tcp_tw_reuse`系统参数混淆，该参数仅在客户端调用connect创建连接时才生效，可以使用`TIME_WAIT`状态超过1秒的端口（防止最后一个ACK丢失）；而`SO_REUSEADDR`是在bind端口时生效，一般用于服务端监听时，可以使用本地非`LISTEN`状态的端口（另一个端口也必须设置`SO_REUSEADDR`），不仅仅是`TIME_WAIT`状态端口。

## 2. 服务端需要限制最大连接数

一个服务端口，理论上能接收的最大TCP连接数是多少呢？TCP四元组中的服务端IP、服务端端口已经固定了，理论上的上限就是客户端可用IP数量*客户端可用端口数量。去除一些IP分类、端口保留等细节，理论上限就是2^32 * 2 ^16 = 2^48。

当然，目前现实中肯定达不到理论上限的瓶颈。一个TCP socket所关联的主要资源有内存缓冲区、文件描述符等，因此，实际限制主要取决于系统内存大小与文件描述符数量限制。

服务端限制最大连接数，主要有两个目的：

- 避免服务过载导致CPU、内存耗尽；
- 避免文件描述符耗尽。

每个TCP连接的socket都占用一个FD，每个进程以及整个系统的FD数量都是有限制的。Linux系统下，通过`ulimit -n`可以查看单个用户的进程运行打开的FD最大数量，通过`cat /proc/sys/fs/file-max`可以查看所有进程运行打开的最大FD数量，如果不符合应用的需求，那就需要进行相应的调整。

达到FD上限会有什么影响呢？首先，肯定是无法接收新TCP连接了；其次，除了TCP连接占用的FD外，你的应用肯定还有内部场景占用或需要分配新的FD，比如日志文件发生轮转创建新日志文件时，如果日志文件创建失败，对于依赖本地存储的应用（如KV、MQ等存储型应用），就导致服务不可用了。所以，要在系统限制的基础上，根据应用的特性预留一定数量的FD，而不能把所有的FD都给客户端TCP连接使用。

有赞在线上压测时，一个应用就碰到过类似的一个问题。压测期间，压力比较高，导致磁盘IO压力增高，请求处理延迟增高，导致客户端超时。客户端发现超时关闭连接，创建新连接重试，但此时服务端由于IO阻塞带来的延迟并未能够及时回收连接关闭（CLOSE_WAIT）的socket以及FD，导致FD消耗越来越多，最终导致FD耗尽，新日志文件创建失败，而该应用又是存储类型应用，强依赖于日志落盘，最终导致服务不可用。

除了服务端限制最大连接数外，如果应用有对应的客户端SDK，最好也在客户端SDK也做一层保护。

## 3. 降低网络读写系统调用次数

当我们调用read/write系统函数从socket读写数据时，每次调用都至少进行两次用户态与内核态的上下文切换，成本比较高。针对该问题，一般有两种优化思路：

- 使用读写缓冲区；读数据时，先一次性从socket读入缓冲区，然后再按需要分次从缓冲区读取；写数据时，先分次写入缓冲区，缓冲区满时或所有写操作完成时，一次性写入socket。
- 当不方便将数据合并到连续内存时，使用readv/writev一次性读取/写入多段内存数据。

对于批量写操作还有一个优点，就是可以避免[Nagle算法](https://en.wikipedia.org/wiki/Nagle's_algorithm)带来的延迟（一般也不建议开启Nagle算法）。假如当前写缓冲区中没有数据，我们先通过write写4个字节，这时TCP协议栈将其发送出去，然后再通过write写96个字节，这时，由于前面发送了一个报文，还没有收到ACK，并且当前可发送数据未达到MSS，Nagle算法不允许继续发送报文，必须等到前一个报文的ACK回来才能继续发送数据，大大降低了吞吐量并且提高了延迟。如果接收端开启了[延迟ACK](https://en.wikipedia.org/wiki/TCP_delayed_acknowledgment)，影响更大。

因此，应该尽量批量读写网络数据，以提升性能。

## RELATED WORK

[有赞TCP网络编程最佳实践](https://tech.youzan.com/you-zan-tcpwang-luo-bian-cheng-zui-jia-shi-jian/)